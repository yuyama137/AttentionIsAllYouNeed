{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predcit\n",
      "epoch : 0, itration : 0, loss : 2.4888906478881836\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([8, 8, 8, 8, 8, 8, 9])\n",
      "eval\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([7, 6, 7, 6, 7, 6, 7])\n",
      "predcit\n",
      "epoch : 0, itration : 1, loss : 2.322777271270752\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([4, 4, 6, 6, 1, 4, 1])\n",
      "eval\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([4, 1, 6, 4, 1, 6, 4])\n",
      "predcit\n",
      "epoch : 0, itration : 2, loss : 1.8938058614730835\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 5, 5, 5, 5, 5, 5])\n",
      "eval\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 5, 5, 5, 5, 5, 5])\n",
      "predcit\n",
      "epoch : 0, itration : 3, loss : 1.9563794136047363\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 1, 4, 4, 1, 4])\n",
      "eval\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([1, 4, 1, 4, 1, 4, 1])\n",
      "predcit\n",
      "epoch : 0, itration : 4, loss : 1.7856427431106567\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 8, 8, 8, 6, 8, 8])\n",
      "eval\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3])\n",
      "predcit\n",
      "epoch : 0, itration : 5, loss : 2.020068407058716\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([8, 2, 8, 3, 3, 3, 3])\n",
      "eval\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([8, 2, 8, 4, 6, 4, 6])\n",
      "predcit\n",
      "epoch : 0, itration : 6, loss : 1.69339919090271\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([2, 7, 2, 2, 2, 2, 2])\n",
      "eval\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([7, 2, 4, 2, 4, 2, 4])\n",
      "predcit\n",
      "epoch : 0, itration : 7, loss : 1.6919440031051636\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7])\n",
      "eval\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7])\n",
      "predcit\n",
      "epoch : 0, itration : 8, loss : 1.7742284536361694\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([1, 1, 1, 5, 5, 1, 1])\n",
      "eval\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 5, 1, 5, 1, 5])\n",
      "predcit\n",
      "epoch : 0, itration : 9, loss : 1.878037691116333\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 5, 5, 5, 5, 5, 5])\n",
      "eval\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 5, 5, 5, 5, 3, 5])\n",
      "predcit\n",
      "epoch : 1, itration : 0, loss : 1.4645429849624634\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 5, 5, 5, 5, 4, 6])\n",
      "eval\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 1, 5, 6, 6, 6, 6])\n",
      "predcit\n",
      "epoch : 1, itration : 1, loss : 1.5488051176071167\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([1, 1, 5, 5, 1, 5, 5])\n",
      "eval\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([1, 5, 1, 5, 1, 5, 1])\n",
      "predcit\n",
      "epoch : 1, itration : 2, loss : 1.3665289878845215\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 5, 1, 5, 5, 1, 5])\n",
      "eval\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 1, 5, 1, 5, 1, 5])\n",
      "predcit\n",
      "epoch : 1, itration : 3, loss : 1.4424874782562256\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 1, 4, 4, 4, 4])\n",
      "eval\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 4, 4, 4, 1, 4])\n",
      "predcit\n",
      "epoch : 1, itration : 4, loss : 1.372459053993225\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 6, 6, 6, 6, 6, 6])\n",
      "eval\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 6, 8, 6, 8, 6, 8])\n",
      "predcit\n",
      "epoch : 1, itration : 5, loss : 1.516083002090454\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([8, 8, 8, 8, 4, 4, 8])\n",
      "eval\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([8, 4, 8, 4, 8, 4, 8])\n",
      "predcit\n",
      "epoch : 1, itration : 6, loss : 1.4721488952636719\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([7, 7, 2, 4, 4, 2, 2])\n",
      "eval\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([7, 2, 4, 2, 4, 2, 4])\n",
      "predcit\n",
      "epoch : 1, itration : 7, loss : 1.4296342134475708\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 3, 3, 3, 3, 3, 3])\n",
      "eval\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 7, 3, 7, 3, 7, 3])\n",
      "predcit\n",
      "epoch : 1, itration : 8, loss : 1.5310931205749512\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 1, 5, 5, 3, 1])\n",
      "eval\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 5, 1, 5, 1, 5])\n",
      "predcit\n",
      "epoch : 1, itration : 9, loss : 1.6639713048934937\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 3, 3, 3, 3, 3])\n",
      "eval\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 6, 3, 6, 3, 6])\n",
      "predcit\n",
      "epoch : 2, itration : 0, loss : 1.196455478668213\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 6, 6, 6, 4, 4, 6])\n",
      "eval\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 6, 4, 6, 6, 6, 4])\n",
      "predcit\n",
      "epoch : 2, itration : 1, loss : 1.4544601440429688\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([1, 1, 5, 5, 1, 5, 5])\n",
      "eval\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([1, 5, 1, 5, 1, 5, 1])\n",
      "predcit\n",
      "epoch : 2, itration : 2, loss : 1.2255111932754517\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 5, 1, 5, 5, 1, 5])\n",
      "eval\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 1, 5, 1, 5, 1, 5])\n",
      "predcit\n",
      "epoch : 2, itration : 3, loss : 1.3620401620864868\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 1, 5, 4, 5, 4])\n",
      "eval\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 4, 4, 1, 4, 4])\n",
      "predcit\n",
      "epoch : 2, itration : 4, loss : 1.3648133277893066\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 6, 8, 4, 8, 6, 6])\n",
      "eval\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 4, 6, 8, 1, 6, 8])\n",
      "predcit\n",
      "epoch : 2, itration : 5, loss : 1.3589683771133423\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([4, 8, 4, 8, 4, 4, 4])\n",
      "eval\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([4, 8, 4, 8, 4, 6, 4])\n",
      "predcit\n",
      "epoch : 2, itration : 6, loss : 1.3119778633117676\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([7, 7, 2, 4, 4, 2, 4])\n",
      "eval\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([7, 2, 4, 2, 4, 2, 4])\n",
      "predcit\n",
      "epoch : 2, itration : 7, loss : 1.2539303302764893\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 7, 8, 8, 7, 8, 7])\n",
      "eval\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 7, 3, 8, 7, 7, 7])\n",
      "predcit\n",
      "epoch : 2, itration : 8, loss : 1.345594882965088\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 5, 1, 5, 5, 5, 2])\n",
      "eval\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 3, 2, 3, 2, 3])\n",
      "predcit\n",
      "epoch : 2, itration : 9, loss : 1.614824652671814\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([3, 3, 5, 3, 3, 3, 3])\n",
      "eval\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 3, 3, 3, 3, 3])\n",
      "predcit\n",
      "epoch : 3, itration : 0, loss : 1.1314059495925903\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 6, 6, 6, 4, 4, 6])\n",
      "eval\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 6, 4, 8, 6, 7, 6])\n",
      "predcit\n",
      "epoch : 3, itration : 1, loss : 1.342454433441162\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([5, 1, 5, 5, 1, 5, 5])\n",
      "eval\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([1, 5, 1, 5, 1, 6, 4])\n",
      "predcit\n",
      "epoch : 3, itration : 2, loss : 1.1026291847229004\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 5, 2, 5, 5, 1, 5])\n",
      "eval\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 1, 5, 1, 5, 1, 5])\n",
      "predcit\n",
      "epoch : 3, itration : 3, loss : 1.234882116317749\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 1, 5, 4, 2, 4])\n",
      "eval\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 5, 1, 4, 8, 4])\n",
      "predcit\n",
      "epoch : 3, itration : 4, loss : 1.1643121242523193\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 6, 8, 4, 8, 8, 4])\n",
      "eval\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 1, 6, 4, 6, 8, 1])\n",
      "predcit\n",
      "epoch : 3, itration : 5, loss : 1.1906970739364624\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([4, 8, 4, 8, 4, 4, 4])\n",
      "eval\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([4, 8, 4, 6, 8, 4, 8])\n",
      "predcit\n",
      "epoch : 3, itration : 6, loss : 1.2586935758590698\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([7, 7, 2, 4, 4, 7, 4])\n",
      "eval\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([7, 4, 1, 7, 2, 4, 2])\n",
      "predcit\n",
      "epoch : 3, itration : 7, loss : 1.1960166692733765\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 6, 6, 6, 7, 8, 6])\n",
      "eval\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 8, 7, 6, 7, 6, 7])\n",
      "predcit\n",
      "epoch : 3, itration : 8, loss : 1.2626566886901855\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([1, 1, 1, 5, 5, 5, 1])\n",
      "eval\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 5, 1, 5, 2, 4])\n",
      "predcit\n",
      "epoch : 3, itration : 9, loss : 1.4914056062698364\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 5, 5, 5, 8, 5, 8])\n",
      "eval\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 8, 6, 5, 2, 3, 6])\n",
      "predcit\n",
      "epoch : 4, itration : 0, loss : 0.928162693977356\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 5, 6, 6, 7, 4, 6])\n",
      "eval\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 7, 6, 4, 8, 6, 7])\n",
      "predcit\n",
      "epoch : 4, itration : 1, loss : 1.171960473060608\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([5, 1, 5, 5, 5, 5, 5])\n",
      "eval\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([1, 5, 1, 5, 1, 5, 1])\n",
      "predcit\n",
      "epoch : 4, itration : 2, loss : 1.027401089668274\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 5, 2, 5, 5, 1, 5])\n",
      "eval\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 1, 5, 1, 5, 2, 8])\n",
      "predcit\n",
      "epoch : 4, itration : 3, loss : 1.1176403760910034\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 1, 5, 4, 2, 4])\n",
      "eval\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 5, 2, 4, 1, 4])\n",
      "predcit\n",
      "epoch : 4, itration : 4, loss : 1.0951067209243774\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 6, 8, 4, 8, 8, 3])\n",
      "eval\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 3, 6, 4, 1, 3, 8])\n",
      "predcit\n",
      "epoch : 4, itration : 5, loss : 1.083448886871338\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([4, 8, 4, 8, 4, 4, 4])\n",
      "eval\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([4, 8, 2, 4, 6, 8, 4])\n",
      "predcit\n",
      "epoch : 4, itration : 6, loss : 1.121995210647583\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([4, 7, 2, 4, 4, 2, 4])\n",
      "eval\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([4, 2, 4, 1, 7, 2, 4])\n",
      "predcit\n",
      "epoch : 4, itration : 7, loss : 1.106813669204712\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 3, 3, 8, 3, 8, 3])\n",
      "eval\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 3, 8, 7, 3, 7, 3])\n",
      "predcit\n",
      "epoch : 4, itration : 8, loss : 1.0722103118896484\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([1, 1, 1, 1, 1, 5, 1])\n",
      "eval\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 1, 5, 2, 4, 2])\n",
      "predcit\n",
      "epoch : 4, itration : 9, loss : 1.247809648513794\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 5, 5, 8, 5, 3])\n",
      "eval\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 2, 3, 6, 5, 8, 5])\n",
      "predcit\n",
      "epoch : 5, itration : 0, loss : 0.8626421093940735\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 6, 6, 6, 6, 6, 6])\n",
      "eval\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 6, 8, 7, 6, 4, 6])\n",
      "predcit\n",
      "epoch : 5, itration : 1, loss : 1.082981824874878\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([5, 1, 5, 5, 5, 5, 5])\n",
      "eval\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([1, 5, 1, 5, 1, 6, 1])\n",
      "predcit\n",
      "epoch : 5, itration : 2, loss : 0.9371181726455688\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 5, 5, 5, 5, 1, 5])\n",
      "eval\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 1, 5, 1, 5, 2, 8])\n",
      "predcit\n",
      "epoch : 5, itration : 3, loss : 1.0500295162200928\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 1, 5, 5, 2, 5])\n",
      "eval\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 5, 1, 4, 8, 4])\n",
      "predcit\n",
      "epoch : 5, itration : 4, loss : 1.1973789930343628\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 6, 8, 4, 8, 8, 6])\n",
      "eval\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 1, 6, 4, 6, 8, 1])\n",
      "predcit\n",
      "epoch : 5, itration : 5, loss : 0.9662861824035645\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "eval\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([4, 8, 2, 4, 6, 8, 5])\n",
      "predcit\n",
      "epoch : 5, itration : 6, loss : 0.9947988390922546\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([4, 4, 2, 4, 4, 7, 4])\n",
      "eval\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([4, 2, 7, 1, 4, 2, 4])\n",
      "predcit\n",
      "epoch : 5, itration : 7, loss : 0.9318803548812866\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 8, 8, 8, 3, 8, 8])\n",
      "eval\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 8, 3, 7, 3, 6, 7])\n",
      "predcit\n",
      "epoch : 5, itration : 8, loss : 0.9655693769454956\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 1, 3, 3, 5, 3])\n",
      "eval\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 3, 2, 4, 1, 3])\n",
      "predcit\n",
      "epoch : 5, itration : 9, loss : 1.17011559009552\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 5, 5, 3, 3, 3])\n",
      "eval\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 6, 8, 3, 2, 3])\n",
      "predcit\n",
      "epoch : 6, itration : 0, loss : 0.8089826107025146\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 6, 6, 6, 6, 4, 6])\n",
      "eval\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 6, 8, 7, 6, 4, 6])\n",
      "predcit\n",
      "epoch : 6, itration : 1, loss : 0.8933200836181641\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([5, 1, 5, 5, 6, 1, 5])\n",
      "eval\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([1, 5, 6, 1, 5, 1, 6])\n",
      "predcit\n",
      "epoch : 6, itration : 2, loss : 0.8278145790100098\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 5, 2, 8, 5, 1, 5])\n",
      "eval\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 1, 5, 2, 8, 1, 5])\n",
      "predcit\n",
      "epoch : 6, itration : 3, loss : 0.9203247427940369\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 1, 5, 5, 2, 5])\n",
      "eval\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 5, 2, 4, 8, 4])\n",
      "predcit\n",
      "epoch : 6, itration : 4, loss : 1.0250606536865234\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 6, 8, 4, 8, 8, 4])\n",
      "eval\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([1, 6, 8, 4, 6, 8, 3])\n",
      "predcit\n",
      "epoch : 6, itration : 5, loss : 0.8730493783950806\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([5, 2, 4, 8, 8, 4, 4])\n",
      "eval\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([4, 8, 2, 4, 6, 5, 2])\n",
      "predcit\n",
      "epoch : 6, itration : 6, loss : 1.0017364025115967\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([1, 7, 2, 4, 4, 7, 7])\n",
      "eval\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([1, 4, 2, 4, 7, 7, 1])\n",
      "predcit\n",
      "epoch : 6, itration : 7, loss : 0.9317686557769775\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 7, 6, 6, 7, 7, 7])\n",
      "eval\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 8, 3, 6, 7, 7, 6])\n",
      "predcit\n",
      "epoch : 6, itration : 8, loss : 0.8822025656700134\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([1, 1, 1, 1, 1, 5, 1])\n",
      "eval\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 3, 4, 1, 3, 5])\n",
      "predcit\n",
      "epoch : 6, itration : 9, loss : 1.0418909788131714\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 5, 5, 8, 5, 3])\n",
      "eval\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 6, 8, 5, 2, 3])\n",
      "predcit\n",
      "epoch : 7, itration : 0, loss : 0.7762852907180786\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 6, 1, 6, 6, 4, 4])\n",
      "eval\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 8, 6, 7, 6, 4, 6])\n",
      "predcit\n",
      "epoch : 7, itration : 1, loss : 0.8975663185119629\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([5, 1, 5, 5, 6, 5, 5])\n",
      "eval\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([5, 1, 6, 1, 5, 6, 4])\n",
      "predcit\n",
      "epoch : 7, itration : 2, loss : 0.8615862131118774\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 5, 2, 8, 5, 2, 5])\n",
      "eval\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 1, 2, 8, 5, 1, 5])\n",
      "predcit\n",
      "epoch : 7, itration : 3, loss : 0.9605129361152649\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 2, 2, 2, 2, 2, 2])\n",
      "eval\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 2, 4, 8, 1, 4, 2])\n",
      "predcit\n",
      "epoch : 7, itration : 4, loss : 0.9036330580711365\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([3, 6, 8, 4, 3, 8, 6])\n",
      "eval\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([3, 8, 4, 6, 8, 3, 1])\n",
      "predcit\n",
      "epoch : 7, itration : 5, loss : 0.9533443450927734\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([5, 2, 4, 8, 8, 2, 8])\n",
      "eval\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([5, 2, 4, 8, 8, 4, 6])\n",
      "predcit\n",
      "epoch : 7, itration : 6, loss : 0.8354552388191223\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([7, 7, 2, 4, 4, 7, 7])\n",
      "eval\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([1, 4, 2, 4, 7, 7, 7])\n",
      "predcit\n",
      "epoch : 7, itration : 7, loss : 0.9375070333480835\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 3, 3, 3, 3, 7, 7])\n",
      "eval\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 3, 7, 3, 8, 7, 3])\n",
      "predcit\n",
      "epoch : 7, itration : 8, loss : 0.8961113095283508\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([1, 1, 1, 1, 1, 5, 1])\n",
      "eval\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([1, 5, 5, 1, 1, 3, 4])\n",
      "predcit\n",
      "epoch : 7, itration : 9, loss : 1.1573573350906372\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 5, 5, 8, 5, 3])\n",
      "eval\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 6, 8, 5, 3, 8])\n",
      "predcit\n",
      "epoch : 8, itration : 0, loss : 0.8681918382644653\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 4, 1, 6, 4, 4, 4])\n",
      "eval\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 6, 8, 6, 4, 6, 4])\n",
      "predcit\n",
      "epoch : 8, itration : 1, loss : 1.0288196802139282\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([1, 1, 1, 5, 6, 1, 6])\n",
      "eval\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([1, 5, 6, 1, 6, 1, 5])\n",
      "predcit\n",
      "epoch : 8, itration : 2, loss : 0.9669084548950195\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 5, 2, 8, 5, 1, 5])\n",
      "eval\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 1, 5, 2, 8, 5, 1])\n",
      "predcit\n",
      "epoch : 8, itration : 3, loss : 1.190580129623413\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 8, 8, 5, 5, 5, 5])\n",
      "eval\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 8, 5, 2, 4, 1, 5])\n",
      "predcit\n",
      "epoch : 8, itration : 4, loss : 1.0047720670700073\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 6, 8, 4, 6, 8, 6])\n",
      "eval\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 4, 6, 8, 3, 1, 6])\n",
      "predcit\n",
      "epoch : 8, itration : 5, loss : 0.9999545812606812\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([5, 2, 4, 8, 8, 2, 8])\n",
      "eval\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([5, 2, 4, 8, 8, 2, 2])\n",
      "predcit\n",
      "epoch : 8, itration : 6, loss : 1.0785952806472778\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2])\n",
      "eval\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([7, 2, 4, 2, 4, 2, 1])\n",
      "predcit\n",
      "epoch : 8, itration : 7, loss : 0.9277019500732422\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 7, 3, 6, 3, 7, 7])\n",
      "eval\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 3, 7, 6, 3, 7, 3])\n",
      "predcit\n",
      "epoch : 8, itration : 8, loss : 0.94792240858078\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 1, 1, 2, 5, 2])\n",
      "eval\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 7, 2, 4, 1, 3])\n",
      "predcit\n",
      "epoch : 8, itration : 9, loss : 0.9007874131202698\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 5, 5, 8, 5, 3])\n",
      "eval\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 3, 8, 2, 3, 3])\n",
      "predcit\n",
      "epoch : 9, itration : 0, loss : 0.8758164644241333\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([7, 4, 1, 7, 7, 7, 4])\n",
      "eval\n",
      "tensor([8, 5, 1, 6, 6, 7, 4])\n",
      "tensor([5, 1, 6, 8, 7, 4, 6])\n",
      "predcit\n",
      "epoch : 9, itration : 1, loss : 1.0797556638717651\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1])\n",
      "eval\n",
      "tensor([4, 1, 1, 5, 6, 1, 5])\n",
      "tensor([1, 5, 1, 1, 5, 6, 1])\n",
      "predcit\n",
      "epoch : 9, itration : 2, loss : 0.9846609830856323\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "eval\n",
      "tensor([1, 5, 2, 8, 5, 1, 5])\n",
      "tensor([5, 1, 5, 1, 5, 2, 8])\n",
      "predcit\n",
      "epoch : 9, itration : 3, loss : 0.9585641622543335\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 1, 8, 5, 2, 5])\n",
      "eval\n",
      "tensor([4, 4, 1, 8, 4, 2, 5])\n",
      "tensor([4, 1, 5, 2, 4, 8, 4])\n",
      "predcit\n",
      "epoch : 9, itration : 4, loss : 0.8273608684539795\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "eval\n",
      "tensor([1, 6, 8, 4, 3, 8, 6])\n",
      "tensor([8, 4, 6, 8, 1, 6, 6])\n",
      "predcit\n",
      "epoch : 9, itration : 5, loss : 1.0027086734771729\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([5, 2, 4, 8, 8, 8, 4])\n",
      "eval\n",
      "tensor([5, 2, 4, 8, 8, 6, 4])\n",
      "tensor([4, 8, 8, 5, 2, 4, 6])\n",
      "predcit\n",
      "epoch : 9, itration : 6, loss : 0.9729417562484741\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([7, 4, 2, 4, 4, 7, 7])\n",
      "eval\n",
      "tensor([1, 4, 2, 2, 4, 7, 7])\n",
      "tensor([7, 2, 4, 1, 6, 4, 2])\n",
      "predcit\n",
      "epoch : 9, itration : 7, loss : 0.9829171299934387\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 8, 8, 6, 6, 8, 8])\n",
      "eval\n",
      "tensor([7, 7, 3, 6, 3, 7, 8])\n",
      "tensor([7, 8, 6, 7, 6, 3, 7])\n",
      "predcit\n",
      "epoch : 9, itration : 8, loss : 0.8102679252624512\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 1, 1, 5, 5, 2])\n",
      "eval\n",
      "tensor([4, 5, 1, 1, 2, 5, 3])\n",
      "tensor([5, 1, 1, 5, 2, 4, 2])\n",
      "predcit\n",
      "epoch : 9, itration : 9, loss : 1.0825093984603882\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 3, 6, 5, 8, 5, 2])\n",
      "eval\n",
      "tensor([2, 3, 6, 5, 8, 5, 3])\n",
      "tensor([5, 2, 3, 6, 8, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# transformer task 1\n",
    "\n",
    "同じid列を再生成するタスク\n",
    "ランダムで仮想文字列を作成して、再生成するタスク。\n",
    "label_smoothing, schedulingなどの学習の工夫に関してはやってない。\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import model\n",
    "import copy\n",
    "\n",
    "max_itr = 100\n",
    "checkout_itr = 10\n",
    "batch_size = 5\n",
    "word_len = 8 # 文章の長さ\n",
    "vocab_num = 10\n",
    "device = \"cpu\"\n",
    "lr = 0.00005\n",
    "batch_num = 10\n",
    "n_minibatch = 8\n",
    "max_epoch = 10\n",
    "\n",
    "def generate_data(batch_size, word_len):\n",
    "    \"\"\"\n",
    "    generate data of virtual sentence\n",
    "    output:\n",
    "        - src (torch.tensor) (batch_size, word_len) : virtual sentence \n",
    "        - tgt (torch.tensor) (batch_size, word_len) : same of src\n",
    "    \"\"\"\n",
    "    data = torch.LongTensor(np.random.randint(1, vocab_num-1, size=(batch_size, word_len)))\n",
    "    data[:,0] = vocab_num - 1 # 0はpadding用 start of sequence は文章中の単語じゃない方がいいかもしれない的なイメージ\n",
    "    src = copy.deepcopy(data[:,1:])\n",
    "    tgt = copy.deepcopy(data)\n",
    "    # tgt_onehot = to_onehot(data, vocab_num)\n",
    "    return src, tgt\n",
    "\n",
    "def make_mask(word_len):\n",
    "    \"\"\"\n",
    "    make mask of tgt data\n",
    "    set 0 where you must not look, otherwise, set 1.\n",
    "\n",
    "    # todo\n",
    "    - we maybe need mask for padding, also.\n",
    "    \"\"\"\n",
    "    mask = np.tril(np.ones((word_len,word_len))).astype(np.float32)\n",
    "    mask = torch.tensor(mask)\n",
    "    return mask\n",
    "\n",
    "# def to_onehot(x, vocab_num):\n",
    "#     e = np.eye(vocab_num).astype(np.float32)\n",
    "#     onehot = torch.tensor(e[x])\n",
    "#     return onehot\n",
    "\n",
    "def culc_loss(loss_func, inputs, targets):\n",
    "    \"\"\"\n",
    "    損失関数の計算\n",
    "    args:\n",
    "        - loss_func : 損失関数(交差エントロピー)\n",
    "        - input (B x len x d): 入力データ\n",
    "        - target (B x len): ターゲットデータ\n",
    "    \n",
    "    文章ごとに平均をとって、バッチごとに平均をとる\n",
    "    pytorchの交差エントロピー使わない方が収束早いし、lossも小さくなる。。。\n",
    "    どういうことだ？\n",
    "    計算結果が微妙に違う気がするし、nn.CrossEntropyが所望の計算をしてない可能性ある？\n",
    "    \"\"\"\n",
    "    B, l, d = inputs.shape\n",
    "    _loss = 0\n",
    "    loss = 0\n",
    "    for i in range(B):\n",
    "        loss += loss_func(inputs[i], targets[i])# 内部的に文章平均\n",
    "        _loss += cross_ent(inputs[i], targets[i])\n",
    "    _loss /= B# バッチ平均\n",
    "    loss /= B\n",
    "    return loss\n",
    "\n",
    "def cross_ent(ipt, tgt):\n",
    "    ans = 0\n",
    "    for i in range(tgt.shape[0]):\n",
    "       ans += - torch.log(ipt[i, tgt[i]])\n",
    "    ans /= tgt.shape[0]\n",
    "    return ans\n",
    "\n",
    "class Batch():\n",
    "    def __init__(self, batch_num, n_minibatch, word_len):\n",
    "        self.data = []\n",
    "        for i in range(batch_num):\n",
    "            self.data.append(list(generate_data(n_minibatch, word_len)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.data[idx]\n",
    "        return src, tgt\n",
    "\n",
    "transformer = model.Model(device, 512, vocab_num, 0.0, 6, 6, 8, 8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=lr)\n",
    "batch = Batch(batch_num, n_minibatch, word_len)\n",
    "batch_test = Batch(1, 1, word_len)\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "for j in range(max_epoch):\n",
    "    for itr in range(batch_num):\n",
    "        print(\"predcit\")\n",
    "        transformer.train()\n",
    "        optimizer.zero_grad()\n",
    "        src, tgt = batch[itr]\n",
    "        # src : (16(B), 7(wordlen - 1))\n",
    "        # tgt : (16(B), 8(wordlen)) 先頭は1(デコーダの最初の入力)\n",
    "        mask = make_mask(word_len)\n",
    "\n",
    "        out = transformer(src, tgt, mask)\n",
    "        # out = transformer(src, tgt, None)\n",
    "        loss = culc_loss(criterion, out[:,:-1,:], src)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"epoch : {j}, itration : {itr}, loss : {loss}\")\n",
    "        print(src[0])\n",
    "        print(torch.max(out[0,:-1,:], dim=-1)[1])\n",
    "        transformer.eval()\n",
    "        # src_, tgt_ = batch_test[0]\n",
    "        src_, tgt_ = batch[itr][0][0], batch[itr][1][0]# 学習したデータすら再現できないので、実装がおかしい？\n",
    "        src_ = src_.unsqueeze(0)\n",
    "        tgt_ = tgt_.unsqueeze(0)\n",
    "        predict = transformer.generate(src_)\n",
    "        print(\"eval\")\n",
    "        print(src_[0])\n",
    "        print(predict[0])\n",
    "        loss_log.append(loss.tolist())\n",
    "        # print(loss_log)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# ransformer.enc_dec.encoder.layers[0].multi_attention.linear_q.weight.grad\n",
    "# transformer.gen.linear.weight.grad\n",
    "# transformer.enc_dec.decoder.layers[5].feedforward.l2.weight.grad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x137e40550>]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-13T15:38:52.362588</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 378.465625 248.518125 \nL 378.465625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 224.64 \nL 371.265625 224.64 \nL 371.265625 7.2 \nL 36.465625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m090b97fd01\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m090b97fd01\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(48.502557 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"113.17141\" xlink:href=\"#m090b97fd01\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(106.80891 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"174.659013\" xlink:href=\"#m090b97fd01\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(168.296513 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"236.146617\" xlink:href=\"#m090b97fd01\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(229.784117 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"297.63422\" xlink:href=\"#m090b97fd01\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(291.27172 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"359.121823\" xlink:href=\"#m090b97fd01\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(349.578073 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m32f6970f29\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m32f6970f29\" y=\"217.79027\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.75 -->\n      <g transform=\"translate(7.2 221.589489)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m32f6970f29\" y=\"188.934713\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1.00 -->\n      <g transform=\"translate(7.2 192.733932)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m32f6970f29\" y=\"160.079156\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1.25 -->\n      <g transform=\"translate(7.2 163.878375)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m32f6970f29\" y=\"131.223599\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.50 -->\n      <g transform=\"translate(7.2 135.022818)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m32f6970f29\" y=\"102.368042\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.75 -->\n      <g transform=\"translate(7.2 106.16726)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m32f6970f29\" y=\"73.512485\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2.00 -->\n      <g transform=\"translate(7.2 77.311703)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m32f6970f29\" y=\"44.656927\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 2.25 -->\n      <g transform=\"translate(7.2 48.456146)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m32f6970f29\" y=\"15.80137\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 2.50 -->\n      <g transform=\"translate(7.2 19.600589)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p945a6aef69)\" d=\"M 51.683807 17.083636 \nL 54.758187 36.256813 \nL 57.832567 85.769649 \nL 60.906947 78.54727 \nL 63.981327 98.254077 \nL 67.055708 71.196144 \nL 70.130088 108.901033 \nL 73.204468 109.068994 \nL 76.278848 99.57154 \nL 79.353228 87.589646 \nL 82.427608 135.316127 \nL 85.501989 125.590403 \nL 88.576369 146.629121 \nL 91.650749 137.861822 \nL 94.725129 145.944659 \nL 97.799509 129.367263 \nL 100.873889 134.438236 \nL 103.94827 139.345375 \nL 107.02265 127.634762 \nL 110.09703 112.297665 \nL 113.17141 166.259384 \nL 116.24579 136.479911 \nL 119.32017 162.905709 \nL 122.394551 147.147231 \nL 125.468931 146.827146 \nL 128.543311 147.501783 \nL 131.617691 152.925533 \nL 134.692071 159.625509 \nL 137.766451 149.045382 \nL 140.840832 117.970282 \nL 143.915212 173.767546 \nL 146.989592 149.407859 \nL 150.063972 177.089024 \nL 153.138352 161.824096 \nL 156.212732 169.969442 \nL 159.287113 166.924032 \nL 162.361493 159.075724 \nL 165.435873 166.310032 \nL 168.510253 158.618293 \nL 171.584633 132.215583 \nL 174.659013 197.226335 \nL 177.733394 169.086652 \nL 180.807774 185.772018 \nL 183.882154 175.356399 \nL 186.956534 177.957284 \nL 190.030914 179.302857 \nL 193.105294 174.853754 \nL 196.179675 176.606042 \nL 199.254055 180.600038 \nL 202.328435 160.331971 \nL 205.402815 204.788867 \nL 208.477195 179.356766 \nL 211.551575 196.192674 \nL 214.625956 183.160195 \nL 217.700336 166.15279 \nL 220.774716 192.826037 \nL 223.849096 189.535043 \nL 226.923476 196.797235 \nL 229.997856 192.908773 \nL 233.072237 169.299593 \nL 236.146617 210.982366 \nL 239.220997 201.247947 \nL 242.295377 208.808738 \nL 245.369757 198.131009 \nL 248.444137 186.042157 \nL 251.518518 203.587637 \nL 254.592898 188.734294 \nL 257.667278 196.810127 \nL 260.741658 202.531156 \nL 263.816038 184.099563 \nL 266.890418 214.756364 \nL 269.964799 200.757837 \nL 273.039179 204.910741 \nL 276.113559 193.492398 \nL 279.187939 200.057601 \nL 282.262319 194.319813 \nL 285.336699 207.926836 \nL 288.41108 196.147791 \nL 291.48546 200.925777 \nL 294.55984 170.772179 \nL 297.63422 204.148305 \nL 300.7086 185.608282 \nL 303.78298 192.754213 \nL 306.857361 166.93753 \nL 309.931741 188.383911 \nL 313.006121 188.939956 \nL 316.080501 179.863071 \nL 319.154881 197.279515 \nL 322.229261 194.945625 \nL 325.303642 200.386051 \nL 328.378022 203.268254 \nL 331.452402 179.729137 \nL 334.526782 190.705177 \nL 337.601162 193.71733 \nL 340.675542 208.861107 \nL 343.749923 188.622072 \nL 346.824303 192.057836 \nL 349.898683 190.906456 \nL 352.973063 210.834012 \nL 356.047443 179.411295 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 224.64 \nL 36.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 371.265625 224.64 \nL 371.265625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 224.64 \nL 371.265625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 7.2 \nL 371.265625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p945a6aef69\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABCx0lEQVR4nO29eXxcV3n//35mn9G+W7LkfbdjZzGOHQeSEEgDBcJOUhqghKa00MIXfhRKv6ULXaB8SykFGtKwb6ENBFJISKAJJCR2EmdxvG/yJmtfR9tIGs35/XHvHc1II2lGHknW6Hm/XnppdO+5c8+R7M995nOe8xwxxqAoiqLkLq757oCiKIoyu6jQK4qi5Dgq9IqiKDmOCr2iKEqOo0KvKIqS43jmuwOpKC8vNytWrJjvbiiKoiwYnnvuuXZjTEWqc5ek0K9YsYJ9+/bNdzcURVEWDCJydrJzat0oiqLkOCr0iqIoOY4KvaIoSo6jQq8oipLjTCv0IlInIo+JyBEROSQiH0rR5noR6RGRF+2vTyWcu1lEjonISRH5RLYHoCiKokxNOlk3UeCjxpjnRaQAeE5EfmmMOTyu3RPGmNclHhARN/Bl4NVAA/CsiDyQ4lpFURRllpg2ojfGNBljnrdf9wJHgKVpvv8O4KQxpt4YMwzcC9wy084qiqIomZORRy8iK4ArgKdTnN4lIvtF5CER2WwfWwqcT2jTwCQPCRG5U0T2ici+tra2TLoFQHQ0xpcfO8njxzO/VlEUJZdJW+hFJB/4EfBhY0x43OnngeXGmG3AvwM/cS5L8VYpC+AbY+42xmw3xmyvqEi5uGtK3C7h7sfr+cWh5oyvVRRFyWXSEnoR8WKJ/PeMMT8ef94YEzbG9NmvHwS8IlKOFcHXJTStBRovutep+8iaynxOtvbNxtsriqIsWNLJuhHga8ARY8znJ2mzxG6HiOyw37cDeBZYKyIrRcQH3Ao8kK3Oj2dNRT6nVOgVRVGSSCfrZjdwO3BARF60j30SWAZgjLkLeCvwxyISBQaBW421R2FURD4IPAy4ga8bYw5ldwhjrKnM54f7ztPVP0xJnm+2bqMoirKgmFbojTG/JbXXntjmS8CXJjn3IPDgjHqXIWsq8wE42dbHy/JK5+KWiqIolzw5tTI2LvRq3yiKosTJKaFfWhwk4HWp0CuKoiSQU0LvcgmryjXzRlEUJZGcEnpAUywVRVHGkXNCv7Yynwvdg/QPRee7K4qiKJcEOSf0zoRsfVv/PPdEURTl0iBnhf5kW+8890RRFOXSIOeEfnlZHm6XqE+vKIpik3NC7/O4WF4WUqFXFEWxyTmhB6vmjQq9oiiKRW4KfWU+ZzsGGBmNzXdXFEVR5p2cFfpozHC2QzNvFEVRclboQWveKIqiQI4K/eoKFXpFURSHnBT6PL+HqkI/ZzoG5rsriqIo805OCj1ASchHeHBkvruhKIoy7+Ss0BcGvIQjKvSKoii5K/RBD+FBLWymKIqSu0KvEb2iKAqQy0If9KpHryiKQhpCLyJ1IvKYiBwRkUMi8qEUbd4pIi/ZX0+JyLaEc2dE5ICIvCgi+7I9gMkoDHjoHYoSi5m5uqWiKMoliSeNNlHgo8aY50WkAHhORH5pjDmc0OY0cJ0xpktEXgPcDVydcP4GY0x79ro9PYVBL8ZA33CUwoB3Lm+tKIpySTFtRG+MaTLGPG+/7gWOAEvHtXnKGNNl/7gXqM12RzPFEXe1bxRFWexk5NGLyArgCuDpKZrdATyU8LMBHhGR50Tkzine+04R2Sci+9ra2jLpVkoKg9aHFc28URRlsZOOdQOAiOQDPwI+bIwJT9LmBiyhvzbh8G5jTKOIVAK/FJGjxpjHx19rjLkby/Jh+/btF22sOxF9r2beKIqyyEkrohcRL5bIf88Y8+NJ2mwF7gFuMcZ0OMeNMY3291bgfmDHxXY6HQoc6yaiEb2iKIubdLJuBPgacMQY8/lJ2iwDfgzcbow5nnA8z57ARUTygJuAg9no+HSMWTca0SuKsrhJx7rZDdwOHBCRF+1jnwSWARhj7gI+BZQBX7GeC0SNMduBKuB++5gH+L4x5hfZHMBkxCdj1bpRFGWRM63QG2N+C8g0bd4HvC/F8Xpg28QrZp+CgE7GKoqiQA6vjPW4XeT53BrRK4qy6MlZoQctg6AoigK5LvRa2ExRFCXHhT7ooVfTKxVFWeTkttBrRK8oipLjQh/0ataNoiiLnpwW+oKARyN6RVEWPTkt9IUBK+vGGK1JryjK4iW3hT7oIWagf3h0vruiKIoyb+S20GtNekVRlBwX+qDWu1EURcltoY/XpNfMG0VRFi+5LfRaqlhRFCXHhV5LFSuKouS40DsevS6aUhRlEZPTQj9Wk14jekVRFi85LfRet4uQ1qRXFGWRk9NCD3YZBLVuFEVZxOS80GsFS0VRFju5L/TB9IS+tTfCwQs9c9AjRVGUuWVaoReROhF5TESOiMghEflQijYiIl8UkZMi8pKIXJlw7mYROWaf+0S2BzAdhYH0Nh/5t1+d4D3feHYOeqQoijK3pBPRR4GPGmM2AjuBD4jIpnFtXgOstb/uBP4DQETcwJft85uA21JcO6uku2/suc4B2vuGGBmNzUGvFEVR5o5phd4Y02SMed5+3QscAZaOa3YL8G1jsRcoFpFqYAdw0hhTb4wZBu61284Zlkc/fUTf2D0IQNfA8Gx3SVEUZU7JyKMXkRXAFcDT404tBc4n/NxgH5vseKr3vlNE9onIvra2tky6NSWFQc+0NemNMTT1RADo7FehVxQlt0hb6EUkH/gR8GFjTHj86RSXmCmOTzxozN3GmO3GmO0VFRXpdmtaCgNeojHD4MjkNenDg1EG7Jr1nX0q9Iqi5BaedBqJiBdL5L9njPlxiiYNQF3Cz7VAI+Cb5PickVgGIeRLPdzGnsH46061bhRFyTHSyboR4GvAEWPM5ydp9gDwLjv7ZifQY4xpAp4F1orIShHxAbfabeeM8YXNUlk4TYlCr9aNoig5RjoR/W7gduCAiLxoH/sksAzAGHMX8CDwWuAkMAD8gX0uKiIfBB4G3MDXjTGHsjmA6RhfqvjP7n0Rt8AXbr0i3qaxOxJ/3aHWjaIoOca0Qm+M+S2pvfbENgb4wCTnHsR6EMwLBQmbj5xu7+d/9jdSWeBPatPUM4jbJYR8bo3oFUXJOXJ/ZaxTwTIywreeOgNAa+9Q0mrZpu4IVQV+Kgr8KvSKouQcuS/09mRsY3eE+55roMKO5uvb+uNtGnsGqS4OUpbno6N/aF76qSiKMlvkvNA7Nem/u/csfUNRPn7zBgBOtfbF2zT1RKgpDlIS8mlEryhKzpHzQu/3uAl4XVzoHmRrbRG3XF6DxyXUt1tCH4sZmroj1BQFKMtXoVcUJfdIK49+oVMY8BIZGeLdu1bgdbtYXhbiVKtl3XT0DzM8GqO6KIDHLXQNjBCLGVyuKeefFUVRFgyLQ+iDXkZjhtdtqwZgdUU+p9qsiN7Joa8uDjJqYDRmCEdGKA755q2/iqIo2WRRCP37r1tNvt+D3+MGYHVlPo8dayU6Govn0NcUBRm0yyB09A+r0CuKkjPkvEcP8Nararl5y5L4z6sr8hkZNZzvGkyI6AOU5lnino5Pf7gxzDX/9L8090SmbasoijKfLAqhH8/qijzAyrxp6ong87goy/PFhT6d1bG/ONhEY08kbgEpiqJcqixKoV9VkQ/AqbY+GrsHqS4KICKU5acf0T91qgMgrU1NFEVR5pNFKfRFQS8VBX5OtVkRfXVRAICSkCP0Uy+aGhiO8uL5bgB6VOgVRbnEWZRCD5Z9c6qtn6buQWqKggAEvG7yfG46+6cW72fPdBGNWVUw09l4XFEUZT5ZtEK/qiKfEy29tPQOUV0ciB8vzfdNG9E/daodr1twiUb0iqJc+ixaoV9dkU84EmU0ZqgpDsaPl+b56ZjGo997qoMr6kooCnoJD06/H62iKMp8soiFPi/+2rFuAMrypi6D0DM4woELPexcXUZh0KsRvaIolzyLWOjz46+TrJtphP6Z053EDFyzusyK6NWjVxTlEmfRCv3S4iB+jzX86nERfUf/cMotBwH2nOrA73FxxbJiCgMa0SuKcumzaIXe5RJWVeST53PHNycBK6Ifjsbot8shjOepU+1sX1GC3+O2PXoVekVRLm0WrdADXLGsmI3VhVj7n1uU2Ktju1LYNx19Qxxt7uWa1eWAtR9tj07GKopyiTNtUTMR+TrwOqDVGLMlxfmPAe9MeL+NQIUxplNEzgC9wCgQNcZsz1bHs8HfvH4zo7Fki6bMKYPQP0xdaSjp3J56azXszlVlgFUVUz16RVEuddKJ6L8J3DzZSWPM54wxlxtjLgf+AviNMaYzockN9vlLSuQBfB4XQZ876dhYYbOJufQPH2qhNM/HttoiwKpzPxyNERlJbfMoiqJcCkwr9MaYx4HO6drZ3Ab84KJ6NM+U5Vl7yo4vbBYZGeXRIy3ctKkKj9v6tRXZ+9GqT68oyqVM1jx6EQlhRf4/SjhsgEdE5DkRuTNb95pNSicpbPbbE+30D48mlTt2Nh7XzBtFUS5lsrnxyOuBJ8fZNruNMY0iUgn8UkSO2p8QJmA/CO4EWLZsWRa7lRl5Pjc+j2uC0D90sJmCgCc+EQsJEb369IqiXMJkM+vmVsbZNsaYRvt7K3A/sGOyi40xdxtjthtjtldUVGSxW5khIvFceoeR0Ri/OtLCqzdW4fOM/cqctEyN6BVFuZTJitCLSBFwHfDThGN5IlLgvAZuAg5m436zTUkoeXXs3voOegZHkmwbSPToNcVSUZRLl3TSK38AXA+Ui0gD8NeAF8AYc5fd7E3AI8aY/oRLq4D77Rx1D/B9Y8wvstf12aMsP1noHzrYTMjn5hXrkj9pqEevKMpCYFqhN8bclkabb2KlYSYeqwe2zbRj80lpno+zHQMAjMYMjxxq5oYNlQS8yamYhQHNulEU5dInm5OxOUNpno/mnggf/a/9xIyhvW+YmzcvmdDO53ER9Lo1olcU5ZJGhT4Fr9xQyZ5THeyt76A3MkJdaZAbNlSmbKsVLBVFudRRoU/By9dW8IsPp5f5Y9W7UaFXFOXSZVEXNcsG87HLVP9QlHP2HIKiKMp0qNBfJPNRk/6L/3uCN33lyTm9p6IoCxcV+otkPjz6fWe76OgfZmQ0Nqf3VRRlYaJCf5HM9b6x0dEYhxvDAPRFdKGWoijTo0J/kRQGvfQNRYnFUm89mG1OtfUzaJdF7lWhVxQlDVToL5LCgAdj5k50D1zoib/uHdJsH0VRpkeF/iKZ6wqWBxOFfh4i+kcONfPUqfY5v6+iKDNHhf4imet6Ny81dJPvt5Y/zIdH/5lfHOWu39TP+X0VRZk5KvQXyVzuMhUdjXG4KcyOlaXA/Fg3beEh+nQlsKIsKFToL5J4YbM5EL9Tbf1ERmLssjcnn2vrZmA4Su9QVCeBFWWBoUJ/kRSF5s66eamhG4Bdq+dH6FvD1obpfUMq9IqykFChv0icXabmogzCwQs95PncbKouxOuWuRf6XlvoNaJXlAWFCv1Fku/34JK5iegPXOhhc00RLpeQ7/fQN8cefUs4AkDf8NytG1AU5eJRob9IRITCOSiD4EzEXlZbBEBBwDtvEb0xMGAv2lIU5dJHhT4LFCWUQWjtjXDtZx9l35nOrN7jZFsfkZEYly11hN4z5xZKa28k/lrtG0VZOKjQZ4HCgDeeXvmLg800dA3y9OnsCv2BBmuh1BZb6PP9nnmbjAXm3DZSFGXmqNBngcSI/uFDzQCcbu+f6pKMOWBPxK4qzwMs62auq2YmRvSaYqkoC4dphV5Evi4irSJycJLz14tIj4i8aH99KuHczSJyTEROisgnstnxS4nCoIdwJEr3wDB7661IPhOh7+wfpn+alMU9pzq4cnkJLpcAtnUzx2mOreEhyvN9gKZYKspCIp2I/pvAzdO0ecIYc7n99XcAIuIGvgy8BtgE3CYimy6ms5cqTkT/v0daGY0ZNlYXciYDoX/nPU/zB994dtJMltZwhBOtfexeUx4/VhCYe+umJRxhVUU+oB69oiwkphV6Y8zjwEwM5x3ASWNMvTFmGLgXuGUG73PJ43j0jxxuZklhgDdsq6GjfzitlMv+oShHm8M8c6aTHz3fkLLNU6c6ALh2nND3DUUxZm7SHCMjo4QjUVZXWNaRWjeKsnDIlke/S0T2i8hDIrLZPrYUOJ/QpsE+lhIRuVNE9onIvra2tix1a24oDHoZisb49bE2btpcxSpbDNOJ6o8292KMJdz/9NBRugeGJ7R58mQ7xSEvm6oL48fy/V5GYyZem362abNTK1eVWxF97zxYNy+c62JgWB8wipIp2RD654HlxphtwL8DP7GPS4q2k4afxpi7jTHbjTHbKyoqstCtucOpYDkUjXHTpiXxCdN0fPojTdZuUf/69svpHhjmcw8fSzpvjOHJk+3sWlUW9+fBejDA3FkozmIp5yE219ZN98Awb71rD/c9l/pTj6Iok3PRQm+MCRtj+uzXDwJeESnHiuDrEprWAo0Xe79LEaeCZVHQy9WrSqkrDSGSntAfbgpTGPBw48ZK3n3NCr7/zDn2n++Onz/TMUBjT4RrEmwbGBP68BwJrrNYqrooSNDrnvP0yoauQUZjhs7+iZ94FEWZmosWehFZIiJiv95hv2cH8CywVkRWiogPuBV44GLvdyni1Lu5cUMlXreLgNfN0uJg2hH9xupCRISPvHod5fl+/u9PDjJqT8w+edLa5OPaSYS+d45SLFvtiL6q0D8vGT+N3YMADAzrilxFyZR00it/AOwB1otIg4jcISLvF5H3203eChwUkf3AF4FbjUUU+CDwMHAE+C9jzKHZGcb8UlsSBOD122rix1aW53GmY2qhH40Zjjb1sqnG8t4LAl4+9bpNHLjQw7eeOgPAU6faqSkKsKIslHRtvt/6FJEouMYYoqOxix5PKlp7h/C4hJKQj/x5yPhxhH66NFRFUSbima6BMea2ac5/CfjSJOceBB6cWdcWDmsqC3jyE69kaXEwfmxleR73v3ABYwz2B54JnO2wNvremDDJ+rqt1dz3XAP/8sgxbtpcxVOnOnjVxqoJ7zEW0Y8J36NHW/nQvS/y5MdfGS+fnC1awkNUFPhxuYQC/zxE9D3WJwqN6BUlc3RlbJZIFHmAFWV59EaidNiesjGGT//sMM8m1MA5bE/EJmbTiAh//8YtjBrD+761j+6BEXavKZtwv1STsQcvhOkbinJ6mk8SM6G1N0JlYQCA/Hmos3NBI3pFmTEq9LPEynEpli819PC1357m848cj7c50hTG4xLWVuUnXVtXGuLDr1rH0eZeAHavTvbnAQr8E3e2arFLFDg2RzZp6x2issAPYJdIVo9eURYKKvSzxMoyS+jrbaF/YL+VcLSnvoNzHQMAHG4Ms6YyH7/HPeH6O65dycbqQjZWF8Yj6UTynYg+QXCdCdPZEPqWcCRB6Oe+RHLco9c8ekXJGBX6WaK2JIjHJZxp72c0Zvif/Y1cXleMCNxnr4A90tSb5M8n4nW7uPcPd/Lt9+5Ied7tEkI+d5LgNseFPpLympkyHI3RNTBClf3Ameusm5HRWDy9c2BII3pFyRQV+lnC43axrDTE6fZ+nq7voLV3iPe9fCXXrinnR8810N43RHM4kuTPj6co5KXCjqJTYdW7SbBu7DLC2Y7o2/qs9x1v3cxV+YXmngjGgEu0mJqizAQV+llkRXkep9v7eWB/I3k+NzduqOJt2+u40D3I1397GmDSiD4dCgLeuPCNjMZotwW5sSe7Qu+siq0stIU+4JnT8gvOg2tFWZ6WQFCUGaBCP4s4ufQPHmjidzYvIehzc9OmKgoDHu6JC33BjN8/cfOR9r4hjAGvW7Ju3TgbjlQWjFk3MHdlEJwH15rKfPp1MlZRMkaFfhZZUZ5HZCRGOBLlDZdbi6kCXje3XL6U4WiMqkI/ZfmTWzPTkViquNnOM99UU0R73xCRLEbbbb3jInq/ncM/RzaK8+BaU5nPcDTGyCwtClOUXEWFfhZxipuV5vmSasm/bXstwJT+fDokevSOP3/lsmJgTPizQUt4CJdAWZ4/fl+Yw4i+e5DSPF/8oagploqSGdOujFVmzkpb6H/3smq87rFn6mVLi3jLlbW8Yt3E/PhMKPCPefTONn9XLCvhG0+eobF7kBX2/UdGY/zgmXOc6xigORyha2CYVeX5XLm8mCuXlbDcTgWdjNbeCBUFftx29cxU5Rdmk8buQaqLAuT5rDTUgeFovJCcoijTo0I/i9QUB/nMmy/jlRsrk46LCP/y9m0X/f7jrRuPS9hi181pTIjonzjRxqd+eoig182SogCFQS8/fr6B7+w9C8A/vfkybtuxbNL7tPYOxf15SLBu5iyij7CsLETIvm+/plgqSkao0M8yt04hoBdLfsDDwPAo0dEYLWFr5WqNXYohMcXyQEMYEXjur15FyGf9yUdjhhOtvXzqJ4f47C+O8tot1ZPWx2kJD1FTNCb0BSkWa0VGRhmNGfL82f8n1dg9yK7VZUkRvaIo6aMe/QKmIGAJc//QqLVytTBAwOumPN+fJPQHG3tYXZEfF3mwFlxtWFLI37xhMz2DI3zx0RMp7xEZGeV0ex/LEqpnOhF9X0IO/9///DDvvOfprI4PrBIPvUNRaooD8f5rLr2iZIYK/QKmwO9sPjJCSzjCEnvl6tLiQLwIGMChCz1xS2c8m2oKecf2Or711Bnq2/omnN9T30FkJMZ168Z2/XKi9kTBPdQY5lhzb9YXUTXZGTc1xUHy/HZEr9aNomSECv0CJtFCaQ5HqLLTH6uLgvGIvqNviMaeCJtriiZ9n4/ctA6/x8U/Pnh0wrlfH20l4HWxc9VYBU2fx4Xf40ry6Bu6BhkcGSU8mN1o2xmHJfS2R6/WjaJkhAr9Asaxblp7h+iNRKmyffSa4iBNPRGMMRxqtEohb146eSpnZUGAD7xyDb860sJT9o5WYJVWfuxYG7tXlxPwJhdeKwh443n0kZHR+ObhTeHsrsp1PpksLQ6SZ1s3ml6pKJmhQr+AcSpYnmy1LJeqAkfoAwwMj9IzOMLBxh6AKSN6gPfuXsmSwgD//ujJ+LFTbf2c6xzg+g2VE9oXJNSkb+gaE/emLK/KbewexOMSyvP9hGzrRmvSK0pmqNAvYArGC33co7cyby50D3LoQphlpaFp884DXjfvvXYFe+o7eKmhG4BfH2sF4Ib1FRPaJ9akb+gaiB9vyuJCLbCEfklRwKrW6XWybjSiV5RMUKFfwDiTsadsoV9SZHv08RTLCAcbe9gyhW2TyG07llHg9/DVx+sBeOxYK+uq8qktCU1om+8fi+jPJ0b0WS6o1tgTiaeMetzW3IB69IqSGSr0CxjHoz9pZ8s4G5TUFFvfjzWHOdsxMK1tk/h+79y5nIcONHG4Mcwzpzu5Yf1E2wYs26g3IaL3uV1UFvhnJaJP3KYxz+/RrBtFyZBphV5Evi4irSJycJLz7xSRl+yvp0RkW8K5MyJyQEReFJF92ey4AgGvC7dL6OwfJuRzxyP88jw/PreLXx6xrJctS9MTeoA/2L0Ct0v44PefZ2TUcEMKfx6wNwi38ugbugZZWhJkaUkwqxH9aMzQ3BOJP7gAQj63evSKkiHpRPTfBG6e4vxp4DpjzFbg08Dd487fYIy53BizfWZdVCZDROI+fVVhABGrFo3LJVQXB9h/vhuAzZPk0KeiqjDAm65YSn17PwV+D1ctL0nZLnGD8IbOAWpLglQXBWYU0Q8Oj/KdPWcYjiZXpWzsHiQaM1QXJUT0Po9aN4qSIdMKvTHmcaBzivNPGWO67B/3ArVZ6puSBmNCn1zuuMYWx+qiAOUZlkK+8xWrAHj5uvKkYmyJJO4y1dA1SG1JiOqioL0bVGaLpr739Fn+6qeHuO+5hqTjzj67iZU/Q373nE/GGmPim7wrykIk2x79HcBDCT8b4BEReU5E7pzqQhG5U0T2ici+tra2LHcrd3EqSVaN20C82rY70vXnE1lTWcAXb7uCj960fvL7BjyMjBq6B0bo6B+OR/QDw5ktmjLG8MNnzwPw7T1n4g+JWMzwg2fOcc3qsngVULAeMHNt3fz6eBvX/79fc1rFXlmgZE3oReQGLKH/eMLh3caYK4HXAB8QkVdMdr0x5m5jzHZjzPaKionpfEpqEq2bRJwJzHQzbsbzhm01rK7In+K+1gPmSLO1IMsSeuueiYumnq7v4C/vP0AsljrKf/5cNyda+9ixspSjzb08fdr68PjEyXYaugYnVNUM+eY+oj9sLzpryvJevIoyV2RF6EVkK3APcIsxpsM5boxptL+3AvcDO7JxP2UMZwJ2vNA7KYlbZhDRZ3LfI029ANSWhFhir8xNXDT1X/sa+N7T53j2TGr374fPniPP5+Y/3nklxSEv33rqDADff/ospXk+btpcldR+Pjz6U3ZWU/fgyDQtFeXS5KKFXkSWAT8GbjfGHE84niciBc5r4CYgZeaOMnMm8+ivXVPOqzZWsWNV6azc16lgebTJinbrSoPx7JjECdkDF7oB+O9x/jtAb2SE/9nfxOu31VCW7+cdL6vjkcMtvHi+m18daeVtV9Xi9ySXXgj53XOeXlnfZlk23QMq9MrCJJ30yh8Ae4D1ItIgIneIyPtF5P12k08BZcBXxqVRVgG/FZH9wDPAz40xv5iFMSxqHAtlybiIvq40xD3v3k5hYHZ2YnLKLxxpDuP3uKjI91OR78clY4umBoajnGztw+sWHjzQNMFb/5/9TQyOjMZr9t++cznGGO789j5GY4Z3vKxuwn3nOqI3xsSrevZoRK8sUKbdJcIYc9s0598HvC/F8Xrg4rdRUqYkfxKPftbva0f0x1v6qC0JIiJ43EJV4ViK5eHGMDEDf3jtSr76m3p+fqCJt28fE+8fPnuODUsK2FZr2Uu1JSFetbGKRw63sGtVGatSzBGEfB4iIzGiozE8k2QEZZOO/mHCdhpp9+DwjN7j2TOdLCsNzfnfSFEcdGXsAmdFmVXHprIwsxTKi8WxjIajMeoSSiQsKQrENyY/cMEqqPaea1awqjwvKX3yhXNd7G/o4R0vq4vn/wO899qVALxr1/KU943XpB+ZG/smMdOmZwbWzchojN+/52m+8tjJ6RsrM+bBA008erRlvrtxyaJbCS5w3npVHa+9rHqClz3b5CdsGVhbMragqboowNFma4L2QEMPFQV+lhQGeMtVtXzu4WOc7ehncGSUO761j6pCP2++InnZxc5VZfz24zekrK8DxHeZGhganTVbKhHHtgn53DPy6Ovb+hmKxpIqfCrZ518eOUZBwMsrN1RN33gRohH9AsftkrhPP5c4lhGQJMqJi6YOXOhh69IiRIQ3X7kUl8A/P3yM2+7ei8/t4t47d6Xcp3YykYexiD7Rp//N8bZZy3Gvb+vH53axYUnBjDz6o3b66YUZpmb2DUU53zkwfcNFjDGGxu4IZzp0ncNkqNArM8LvceOzPfLxEf3A8CjN4Qgn2/ridXaqi4Jcu7aCn7/URMDr5t47dyYthEqXxIje4U+//zyf/+XxyS65KE619bO8LERpnn9G6ZVO+ulMi73926+O87a79szo2sVC18AIgyOjdA+M0D0ws3mUXEeFXpkxjk9fV5oc0QP86kgrxsDW2rE8/g9cv5qdq0q5986drJiByMPEiL43MkI4EuWIneaZberb+1hVkUdR0EvPDETE6VfP4MiMVvQeb+mjORwhOhqbvnGOMDIa44Pff56D9hzPdDQmfFrS1cupUaFXZoxj3yRG9M6iqUcONQNwWULlzKtXlXHvnbtYXjYzkQcSthO0RLMlbEXK9W19RLI8QRsdjXGuY4BVFfkUh7wztm5CPuvhNJPKnuftTV0WU2pnQ9cgP3upid8cT68USuL8x9kOtblSoUKvzJh8v4eg101Zni9+zFk0tedUB1WF/niN/GwRj+ht68axRGIGjtmTwNnifJdVPXNVeR7FQS/9w6MTKmxORWf/MC3hIa61i7JdyHCbxVjMxEWsaxEt1nIe3s4+xNOhEf30qNArMybf74nn0Ds4i6aiMZMUzWcLx6N3bJBE7zvb9o2TceNE9JBZZO1MxL7Srumfaa2c1t6h+INlMXnPcaHvS0/oL3QPEvS6WVoc1AnZSVChV2bMHdeu5M9uXJt0zON2xRcGXba0OOv3dKybfruwmZOzH/S6Jwh9z+AIvZGphfmxY6187L/3p4wEndIHqyvyKAxmLvTOROx16ytwSXLkmQ7nE/biXYwRfXsGEX1NcYAV5SHOqHWTEs2jV2bMTZuXpDy+xN6A5LLamVXOnIqg7XcPJET05fk+lpWG4sLq8Iff3sep1j7uftf2lBuoNPUM8qEfvEA4EuUnL17g3btW8Kc3ro1vpF7f3kdJyEtxyEdxyLKnejJYHXu0KUx5vo/qoiCVBYGMrZvEtMquRRXRWwKfSUS/tCREXUmQn73UNJtdW7BoRK9kHWfTk0y2MEwXn8eFz+1KiOgHWVIUYFNNIUeaw/F69m29QzxzupOewRFu+8+9/PTFC0nvY4zhz+97iZFRw3/90S7efEUtX3vyNK/+/G/ikXd9W3+8DEOxLf6ZLJo62tzLxmrrYVdTHMh4MvZcgtAvSusmg4h+aXGAleV59AyO0NW/eH5X6aJCr2Sdq1eVsnNVKZUFs1PbxdplaiyiX1IYZGN1Ib2RaHzy8rFj1n6533rvDi6vK+ZD977I3/3P4biIfGfvWZ440c4nf3cjO1aW8tm3buX+P9nNwPAof/K95xmOxqhv72eVnQZalKF1Ex2Ncbyllw1LCgCoLg5mbt10DlJV6MfjkkVl3bTaEX1vJDptJlVkZJT2vmGWFgfj2Vzq009EhV7JOu/atYJ779w1a++f5/PEs25awhGqiwLxyNnx6R872sqSwgDXrC7ju3dczW07lvGNp06z+zOP8sHvP88/PniE69ZV8PtXj21scnldMf/81q28eL6bT95/gLbeobGIPpQ6om/oGki5deKZDqv0wYYlVr+WFgdpzHCbxfNdAywrDVEc8i2uiL43gjO/3z6NfeM8PGuKg6wst9ZzLFShb+gaiCcAZBsVemXBkWdH9JGRUboGRlhSFGDDkgJE4HBTmOFojMePt3HDhkpEBJ/HxT+9+TJ+/f9dz+27lvPrY20EvG7++a1bkzKGAF57WTXv3b0yXoBtVYUVJRYEvIgkbz5yvnOAV/zzY/zqSOuEPjrzBc4DqLoowHA0RkcGtsL5zgHqSkKUhLyLpha+MYaWcISVdnTe3jf178spLbG0OEhdaQgRON2eekL2yZPtM15B3TMwwm137501IQb4yq9P8dZZWgWtQq8sOEI+D/3Do/GMmyWFAUI+DyvK8jjSFOaZ0530D49yo53W6LC8LI+/fv1m9n7yRn71kesmLRv8F6/dEJ+8XW0LvdslFAa8hBOE/mRrHzEDz5/rmvAeR5vDeFzC6krremfHr6Y0J2SHolYZibrSECUh36KZjA1HokRGYmy253em8+kTI3q/x01NUZCzk0T0//rL4/z7oyfitl8mvNjQzZ76Du5/4cL0jWdIa3iIyoLZqUKrQq8sOPL8bvqHovEc+mp7Ne6m6kKONPXy6NFWfB4X16wpS3l9vt9Def7k/6G8bhdfvf0qPvPmy5L2zS0KepMslAZbZFLl7x9t6mV1RX68qqizh2+6xc0auyMYY5WXKF5EEX2rPYeypcb6JDSddXOhaxCXjK3IXlmex5kUqbKN3YPsO9uFMVZZiUxxJsbTXa07E9p6I1So0CuKRcjnoX8oSrO9Cbnzn3xjdQHnOgd46GAT16wuiy+umgnl+X5u3bEsydopDnmTrJsLXZML/ZGmMBuqC+I/Ow+jdDNvHGGpKwkuqojeSa3cZAv9dBH9he4IVYUBvHaBvRXlIU6390+YC3nwwFja5dEZLKxrsP8eBy700JFm2memtPYOzVoCgwq9suDI87kZGB6NR/RjQm+JQ1NPJL4aNZtYEX2C0NvReUt4iM4E7729b4jGngibqsfWEZTm+fB7XGln3jg59MvKrIi+a2Ako4ncbLDvTGfWy0pMh5MVVVdibagzvdAPxD8tAawoyyMciU74BPSzl5rYWF1IyOeO75eQCec6B/C5XRgDvz3ZnvH10xGLGdp6h2ZtAyEVemXBEfJ7GBiO0twToSjojUfuGxOE9Yb1syP04aSIfgCfx/ovlBgl7jtjefbbV4wt0hIRauzMm3Q432UJS1VBgOKQj+FojME52lXL4cM/fJHPPXx0Tu/Z0mv9fioL/VQU+NPIuonE5z/AEnqA0wk+/fnOAV48380bttWwtqpgRg+v810DXL2qlJKQl98cy7590zUwTDRm1KNXFIc8n5v+ISuidywRsOyRoqCX9VUFSaWTs8UE66Z7kN2rrXmAw0lC34nf45qwYKymOJBRRF9bEsTlEkrs1M5Mc+mjozG+/tvTM5p87B+y1iQ0h2dWR3+mtPREKAh4CPk8VOT7J0T0t969h3ueqAesKLipZzBZ6O11D4k+/c9t2+Z1W6vZUFXAsZbejD8dne8cZEVZHi9fW8HjJ9qJxbL76arVHue8WTci8nURaRWRg5OcFxH5ooicFJGXROTKhHM3i8gx+9wnstlxZfES8nkYHBnlQtdg3LYBK2r+i9ds4M9vXj8r9y0O+ugZtCyU4WiM1t4httYWU1HgTyq/8OyZTrbVFU/Y3rG6KJh21s35zkFq7YeVU34h01z63xxv4+9+dpiHDjRndB3AiVZrwtLxzOeKlvBQPBuqfFxE3z0wzN76Tr782EkiI6O09Q0xMmpYmlAmu640iEtIqnnz85ea2FZXTF1piA3VBXT2D6ddXgGsRXI9gyPUlQa5bl0F7X1DSQ/2bBAX+nm0br4J3DzF+dcAa+2vO4H/ABARN/Bl+/wm4DYR2XQxnVUUGCtVfLq9PymiB7h1xzJu3Dg7+4YWBb2Mxgx9Q1GaegYxBpaWBNmwpCBeqXJgOMrBxjAvWzGxtk5NcZCW3ggjaWwicr5rgDpbwEomWaw1HU+d6gDgWEvmVsVx+5r2vqE53fSkpTdClS124yP6k/bDp2tghAf2N8ZXQS8tHvs34Pe4qSkOcqKll1jMcKa9nwMXenjdZdUArLdXKh9tSv93Ep8vKQ3x8nVWyelsZ9842UZV8xXRG2MeBzqnaHIL8G1jsRcoFpFqYAdw0hhTb4wZBu612yrKReF48oMjoywpDE7TOnsUJQiuk3FTWxxkU3UhJ1r6GBmN8cK5bkZjhpetKJ1wfU1RAGPGJhyNMSkthHBkhO6BEZbZEX2JXe8/08ybPbbQj598HBmN8d5vPstTU0wqOqJqDBkt8rpYWpMieh/9w6Nx68lJi6ws8POtp84kLJZKtunWVubz0MFmtvzNw7zznqcB+N2tltA7K5Uz8ekdoa8tCVFZEGBTdSGPpyH07/nGM/z1Tw+mZRNdChH9dCwFzif83GAfm+x4SkTkThHZJyL72tpmL1dVWfjk+8fSJsdH9LNJcUK9GyeHfmmJVWdneDRGfVs/z57pRASuTFEt0/GSG7sjdA8M88YvP8nH7ntpQjtHWOri1k3mHn33wDBHmsOIwPFxonas2Vpr8OVfn5z0+uMJnwJa5sinj8UMrb2RuNBX2Gsd2nutB82J1l6CXjcfetVaDjWGecAuVFdTnPxv4DNv2cpn33IZb99ex9KSIO+8eln8d1+a56OiwJ9R5o1TLtr5e1y3voLnznZNWQJ7KDrK48fb+Naes/xrGqtx23qHKAh4CHjd07adCdkQeklxzExxPCXGmLuNMduNMdsrKiqy0C0lV3G25gOSPPrZZqxUsRXRi1i+u5Mvf7Q5zL4zXWxcUkhhwDvhekeQTrb28QfffJb9DT38+PmGpHLEYPnzYKUYgjU3ANCdQWS9t74TY+DGDVU0hyP0JDwkXmqw9mJ96lTHhHs7nGjpixd0myufvmtgmJFRQ5WdeVJuf2/ri8T7tLYqnzdfUUtR0MuvjrRSGPBQMO53XVUY4B0vW8bfvGEz//VHu/iHN12WdH7DkgKOtaTvsZ/rHKAo6I0XtrtuXQXRmJlyley5jgFiBlaUhfjioyf57t6zgPWQ/cxDRydUU20JR2Yt4wayI/QNQF3Cz7VA4xTHFeWiyJuniL4ooVTxhe5BKgv8+DwuVlfk43O7ONDQw/PnulL681Zfrajy0z87zEsNPfztGzYjInz36bNJ7RI9YbBKM+f53BlF9HvrOwh63bx9ey0wttsVwEsN3fGH5Y+eb5hwbd9Q1MoosrdAnKuI3nmgjI/o2xIi+jWV+QR9bm7dYUlLYsZNumxYUsCJlj5G08ycOd85SF3p2H22Ly9h16oy/uaBQ/GaSOM5ZW9a8y9vv5xXbqjkUz89yM1feJzf+cLj3PWbU3zp0eRPU7O5WAqyI/QPAO+ys292Aj3GmCbgWWCtiKwUER9wq91WUS6K+YvobaEfHOZC12B8oY7X7WJNZT4/3d/IwPAo21P482A9oIqCXgZHRvnnt2zl3des4KZNVfzw2fNJ5Xjr2/spCHjicwLWvTOrYLnnVAfbV5RwWa2V4pk4Ibu/oYftK0rZvbqc+55rmJAq6Pjz16wuQ2TMP54OJxtppozl0NtCH4/oh+gZHKElPMS6KuvT0+07l+OS5I3p02X9kkKGorG0q1w6VUQdPG4XX3/Py9i1uoyP3befHz57bsI19e3W73BdVT5f+r0r2LmqDI9b+OvXb+K2HXWc6ehPmuRu7Y3Mmj8P6aVX/gDYA6wXkQYRuUNE3i8i77ebPAjUAyeB/wT+BMAYEwU+CDwMHAH+yxhzaBbGoCwynIg+3z/xY/tskliT3tnVyGFjdWE8QyTVRKzDna9YxefeupW3XGVF2u++ZgXdAyM88KL1Yff5c13c99x5Xr62POm6kjxv2pOx7X1DHGvpZeeqMpYUBigMeOKe9ODwKMdbetlWW8TbttfS0DXI3vqOpOsdf35jdSFlef54Rsh0fPXxel7+z4/SN5Sctx+LGU62Tu+JxzNPbMErzfMhYm0p6Fy/ttKqPVRbEuJvb9nCe65ZmVbfEtmQQeZNLGZo6ByM22gOQZ+br737Zbx8bQUf/9EBHjuaXMG0vq2figI/BQFrQd/3/3AnP/vTl/MHu1dyxbISRkbHNn43xsxqQTNIL+vmNmNMtTHGa4ypNcZ8zRhzlzHmLvu8McZ8wBiz2hhzmTFmX8K1Dxpj1tnn/mHWRqEsKpyIfi6jeYCA143f46Krf5imnsGkpfcbbZ++rjQ4Zb8+cMMa3rZ9zNG8emUp66sK+OZTZ2jrHeKPv/sc1UVB/nGcr1wS8iUt1poKR7h3rS5DRNiwpDCeZXK4qYfRmGFrbTG/s3kJBQEP/z3OfjjR0ovf46KuNERVoT9t6+ahg820hIe495nkCPfLj53k1f/6+KTzAQ6OdeNYGF63i9KQj7a+IU60OBHyWP2g23cu59pxD8R0WFOZj0vgmG1nPX+ui88/cixl2mtr7xDDo7GUC/ACXjd3334VhQEPjxxOXqtQ3zY2xzEepyLqKbvkcTgSZSgau+StG0WZU5wNwpdMUmZ4NikOeTnR2jdhoY5TfmGqaD4VIsK7r1nB4aYw7/jqHnoGR7jr96+KT/yO3deXdh79nlMd5Ps9bLVX5q5fUsDxZms16P7z1kTs1toiAl43b9hWw4MHmggnZJCcaO1jdUU+bpdQWeBPy7rpGRjhQEM3IvCfT9QzFLWsqI6+Ie76zSmMgWdOJ2dpj8YMD+xvjLdtCUcoy/PFy0qAVVyuvXeI4y19BL3upIfrTAl43awsz+NIcy/3PFHP2+/awxcfPcn3n55owZwblwGV6r221hbHJ7gdTrePbUM5nlXl1nFn8/m2hLIPs4UKvbLgCPnnJ6IHKwPmUKMVCdYmiM6WmiIK/B5u3JD5Yq03XlFDQcBDfXs/n33L1njlxkRKQulbN3vqO3jZihI8dkXH9UsK6B2K0tgTYX9DN1WF/viE59u21zEUjfHTF8fyJE609LGuyhKjqsJAWlk3T51qJ2bgT29YQ0t4iJ++YL3flx47SSQaI+h189y4uv2PHm3lz37wQnwzkJbwUNyfd6go8FsRvT0R63KlSubLnA1LCvnl4Rb+/udHeOWGSq5eWcoXfnU8KTsJElJdp5gLuKy2iGPNvfF5lq7+YboGRuKR+3hK8nyU5vniEb2zdeJslSgGFXplAeJzu6guCrA5hSDONkWhsYqKiRF9UcjL8596dXxhTiaEfB7+8U2X8ek3buGWy1MvNSkOWeUXpssUaQlHqG/rZ9fqsVr8jid9rDnMSw09bK0tjp/bVlvEtrpi/sMuK+Bk3Ky1LZLKwgAd/dOvjn3iZDv5fg9/euNaNtcUctfjpzjb0c93957l7dtrednKUp4/myz0T9oLtu554jQHL/TQEh5bFetQnu+jrdeybhx/PhtsX1GCxyX81es28dXbr+KvX7+Z7sER/v3RE0ntzncNIJL8tx7P1qVFRGMmPg/iTMSumkTowbJvnIh+tuvcgAq9sgAREX79set5964Vc35vZ0IWmGAjODXRZ8Lrt9Vw+87lk54vDnoxhqTqman47QlLPK9ZPeZdr7OF/pnTXZxu72db7VixNRHh4zevp7Enwnf2nOVES/KkZ1WhH2Om39Lvtyfa2bmqFK/bxR9fv5r6tn7e841ncYnwoRvXcdWyEo619CZZRE+ebOeKZcWU5vn48/teoqlncEIJgIoCP809EZrDkfjDJxu8a9cKnvurV3PHtSsRETbVFPKO7XV8a88ZTicURDvXOcCSwsCEukWJOJlNBxq6gbHUypXlkz+YVpXnj0X0at0oSmr8HnfWPsZngrM6tjjkTcrnn21K8pzVsVML7q+Pt1Ge70+qhV8Y8FJTFOD+F6xJ18SIHqyHwivWVfClx07ynB11O5OeTpQ51YTsuY4BznUOcK2dd/+aLdUsL7M2APmD3StZUhTgquUlGAMvnusGrAybE6193LRpCZ++ZTOHm8K09w2niOj9RO1PMdmM6N0uSXpoA3zkpnX43C4+89CR+LGGzsFpK6EuLQ5SmueL+/T1bf143TKl3bO6Mo+O/mF6BkZoDQ8R8LoomMV/Tyr0ipIBTi59NiYFM7uvU+/GiogjI6O8MM7zjo5am6Jfv75iwkNw/ZKCuNe+tTa5fDLAx29eT8/gCF/41Yl4xg2MpTomCv139pzhr34yVsPliZNWyZJr11or2t0u4aM3rWdtZT5/fN1qALbVFeES4g+SPXZm0O41Zdy8pZqbNy8BSOnRO6ytyp7Qp6KyIMCf3LCGhw+18B+/PgVYEf341MrxiAiXLS3iwAVH6PtYVhqKz5GkwpmQPdXeF18sNX6j+myiQq8oGeBEgXMt9CXjShXf80Q9b/rKU0nbGO5v6KZncITr108sIbLeLua1vCw0IaMHYHNNEW+8vIa+oWg84wbGVqkmZt58d+85vrP3LN948gxgWTDVRYGkycc3bKvhlx+5Lr7oqyDgZf2SwvhG6k+ebKcw4GFzjfXQ+btbNvOKdRXsXJWcteTs7RvwuqidRnCzwZ2vWMXrt9Xw2V8c5R9+fpiW3kjSYqnJ2FpbxPGWXgaHR6mfIuPGYbX96eRUa5+1WGoWJ2JBhV5RMqLIFsmZLL2/GMaXKn7ooJW3nZgS+NjRNtwu4eVrJgq9MyE73rZJ5KM3rcfrlngpX4Aye9GSs5ipNzLCcbu42D89dIQXznXx5MkOrl1TPm1EeuWy4nh1zydPdrBrdVn8gVJZGODb793BmspkH96J6BMfPrOJ1+3iC++4nN+7ehn/+cRpe4P26f/Wly0tImasPWXPdvRPORELVhaP1y3Ut/dbEf0s+vOgQq8oGeF49DNZen9R9w2NlSo+3znAocYweT43979wgX57Jepjx1q5allJUukEB6fw2rYUto1DXWmI795xNR959br4MY/bRXm+P2777D/fgzHw2bdupSLfz3u+8Sw9gyNpLVy6ankJfUNRfnm4JamWzlQ4Qr8uixOx0+F2Cf/wxi380XWrAJIefJPhPEAfPNDEyKhh9RQTsWD9XpeX5XGqtY+28OzWuQEVekXJiPny6AsDHtwuoXtghEcOtwDwd7dsoW8oygP7G2kNRzjUGOb6Dakrv66vKuDzb9/GrTuWTXmfq1eVTZh8rCr0xzNDnHmB69ZV8O+/d0W83EE6on2VXbr5S49ZKYzXJKSATkZJyMey0lBSuuhcYO1WtpF9//dVcXtpKqrsPW5/9pK1fmC6iB5gVXkehxrD9A5FZz2in7u0AUXJAa5cVsK7di1n9wyW3l8MIkJx0Fo09cyZTtZXFfDmK5fyn0/U8929Z+O2xvXrUm+KLiK8+craGd27siBAs72p+Qvnu1lTmU9R0MtVy0v59C1bONIUjnvpU7GsNER5vo+DF8JUFvhZPY2PDVZ0/fif3zCjfmeDdMYF1u9369Ii/teueTOdRw+WT+88tDWiV5RLiDy/h7+7ZUvKevOzTVHIy6m2Pvad6eR3NlchIrxz53IONYb56m9OUVXoj9fcySZORG+M4YVzXVy5rDh+7veuXsan37glrfcREa5cZkX1u9Pw9Bcajn1THPJSmjdxwns8ibVwdDJWURTAsjH21ncSM3CTnY74xstrCPncnGrr54b1lbMinpUFATr6hznV1kfXwAhXLEtdbz8dHPsmHdtmoeGkrU5WzGw8qxPWBehkrKIowFjmzdLiYLz8Q0HAGy+bcP361LbNxVJVaO1169gMVyRE9Jnymi3VvHxt+axt4D6fbLGLyE21IjaRxAnb2bZu1KNXlAWCk3lzk23bOPzJ9asxxnDdutnZgtOxFR4+2Ey+38PaypnbQ8vKQnznjquz1bVLiooCP3dcu5IbN6T3wC0KeSnPt2oYlaTIlMomKvSKskBwxOB3bNvGoa40xGfesnXW7ussmtrf0MPuNWVzks++UPmr123KqP2q8nwaugZmfb5ChV5RFgjXrCnnZGsf25fP3COfCYn1Z66om9t75zrv2b1iTvbkVaFXlAXCDesruWGWfPipKMv34xKImYvz55WJvPayzMtazwSdjFUUZUrcLonnk19Mxo0yf2hEryjKtFQVBgj53GnlhyuXHmkJvYjcDPwb4AbuMcZ8Ztz5jwHvTHjPjUCFMaZTRM4AvcAoEDXGbM9S3xVFmSM+cMNqptncSrmEmVboRcQNfBl4NdAAPCsiDxhjDjttjDGfAz5nt3898H+MMYk7Ad9gjGnPas8VRZkzbt4yN16yMjuk49HvAE4aY+qNMcPAvcAtU7S/DfhBNjqnKIqiXDzpCP1S4HzCzw32sQmISAi4GfhRwmEDPCIiz4nInZPdRETuFJF9IrKvra0tjW4piqIo6ZCO0KfK5J/MrXs98OQ422a3MeZK4DXAB0TkFakuNMbcbYzZbozZXlExOyv8FEVRFiPpCH0DUJfwcy3QOEnbWxln2xhjGu3vrcD9WFaQoiiKMkekI/TPAmtFZKWI+LDE/IHxjUSkCLgO+GnCsTwRKXBeAzcBB7PRcUVRFCU9ps26McZEReSDwMNY6ZVfN8YcEpH32+fvspu+CXjEGNOfcHkVcL9dx8EDfN8Y84tsDkBRFEWZGjHm0kuO3b59u9m3b998d0NRFGXBICLPTbZOSUsgKIqi5DiXZEQvIm3A2RleXg4stsVZi3HMsDjHvRjHDItz3JmOebkxJmXK4iUp9BeDiOxbbGUWFuOYYXGOezGOGRbnuLM5ZrVuFEVRchwVekVRlBwnF4X+7vnuwDywGMcMi3Pci3HMsDjHnbUx55xHryiKoiSTixG9oiiKkoAKvaIoSo6TM0IvIjeLyDEROSkin5jv/swWIlInIo+JyBEROSQiH7KPl4rIL0XkhP095zb3FBG3iLwgIj+zf14MYy4WkftE5Kj9N9+V6+MWkf9j/9s+KCI/EJFALo5ZRL4uIq0icjDh2KTjFJG/sPXtmIj8Tib3ygmhT9gF6zXAJuA2Edk0v72aNaLAR40xG4GdWKWfNwGfAP7XGLMW+F/751zjQ8CRhJ8Xw5j/DfiFMWYDsA1r/Dk7bhFZCvwZsN0YswWrvtat5OaYv4m1f0ciKcdp/x+/FdhsX/MVW/fSIieEnsx3wVqwGGOajDHP2697sf7jL8Ua77fsZt8C3jgvHZwlRKQW+F3gnoTDuT7mQuAVwNcAjDHDxphucnzcWAUQgyLiAUJYZdFzbszGmMeBznGHJxvnLcC9xpghY8xp4CQZlHzPFaFPexesXEJEVgBXAE8DVcaYJrAeBkDlPHZtNvgC8OdALOFYro95FdAGfMO2rO6xy33n7LiNMReA/wecA5qAHmPMI+TwmMcx2TgvSuNyRegz2QUrJxCRfKwtGz9sjAnPd39mExF5HdBqjHluvvsyx3iAK4H/MMZcAfSTG5bFpNie9C3ASqAGyBOR35/fXl0SXJTG5YrQZ7IL1oJHRLxYIv89Y8yP7cMtIlJtn68GWuerf7PAbuANInIGy5Z7pYh8l9weM1j/rhuMMU/bP9+HJfy5PO5XAaeNMW3GmBHgx8A15PaYE5lsnBelcbki9GntgpULiLWLy9eAI8aYzyecegB4t/363STs9LXQMcb8hTGm1hizAutv+6gx5vfJ4TEDGGOagfMist4+dCNwmNwe9zlgp4iE7H/rN2LNQ+XymBOZbJwPALeKiF9EVgJrgWfSfldjTE58Aa8FjgOngL+c7/7M4jivxfrI9hLwov31WqAMa5b+hP29dL77Okvjvx74mf0658cMXA7ss//ePwFKcn3cwN8CR7G2Hf0O4M/FMWPtr90EjGBF7HdMNU7gL219Owa8JpN7aQkERVGUHCdXrBtFURRlElToFUVRchwVekVRlBxHhV5RFCXHUaFXFEXJcVToFUVRchwVekVRlBzn/wdNwpZM3yt0EAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.8738328218460083"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "loss.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}